{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def trainingModel(trainingFile):\n",
    "    lexical_prob=dict()\n",
    "    trans_prob=dict()\n",
    "    start_1=0\n",
    "    start_0=0\n",
    "    count=[]\n",
    "\n",
    "    with open(trainingFile) as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        next(readCSV, None)\n",
    "        for row in readCSV:\n",
    "            s=row[0].replace('[',' ').replace(']',' ').split()\n",
    "            m=[int(i) for i in row[2].replace('[',' ').replace(',',' ').replace(']',' ').split()]\n",
    "            dict_iteration=dict(zip(s, m))\n",
    "            for i in dict_iteration.items():\n",
    "                if i in lexical_prob:\n",
    "                    lexical_prob[i]+=1\n",
    "                else:\n",
    "                    lexical_prob[i]=1\n",
    "\n",
    "            if(m[0]==0):\n",
    "                start_0+=1\n",
    "            else:\n",
    "                start_1+=1\n",
    "            for i in range(len(m)-1):\n",
    "\n",
    "                tup=(m[i],m[i+1])\n",
    "                if tup in trans_prob:\n",
    "                    trans_prob[tup]+=1\n",
    "                else:\n",
    "                    trans_prob[tup]=1\n",
    "        count.append(trans_prob[(0,0)]+trans_prob[(0,1)])\n",
    "        count.append(trans_prob[(1,0)]+trans_prob[(1,1)])\n",
    "        return lexical_prob,trans_prob,start_1,start_0,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(output,actual):\n",
    "    print(accuracy_score(actual,output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fscore(output,actual):\n",
    "    print(f1_score(actual, output))\n",
    "    print(precision_recall_fscore_support(actual, output,average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Features used- word, part of speech of that word, length of the word,position of the word , part of speech of previous word, \n",
    "#part of speech of next word\n",
    "def get_train_tuple(word,pos,tag,length,word_position,prev_pos,next_pos):\n",
    "    feature_dict = {\"POS\":pos, \"word\":word, \"length\":length, \"position\":word_position, \"prev_pos\":prev_pos, \"next_ps\":next_pos}\n",
    "    return (feature_dict,tag)\n",
    "\n",
    "def get_test_dict(word,pos,length,word_position,prev_pos,next_pos):\n",
    "    feature_dict = {\"POS\":pos, \"word\":word, \"length\":length, \"position\":word_position, \"prev_pos\":prev_pos, \"next_ps\":next_pos}\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import maxent\n",
    "def getTrain():\n",
    "    ksTrainPath = 'C:/Users/hp/Desktop/Kaveesha/Cornell/Fall 2019/5740-NLP/P2_release (3)/P2_release/data_release/train.csv'\n",
    "    train = []\n",
    "    with open(ksTrainPath) as csvfile:\n",
    "            readCSV = csv.reader(csvfile, delimiter=',')\n",
    "            next(readCSV, None)\n",
    "            for row in readCSV:\n",
    "                s=row[0].replace('[',' ').replace(']',' ').split()\n",
    "                m=[int(i) for i in row[2].replace('[',' ').replace(',',' ').replace(']',' ').split()]\n",
    "                p = row[1].replace('[',' ').replace(',',' ').replace(']',' ').replace('\\'','').split()\n",
    "                for i in range(len(s)):\n",
    "                    #associate start pos with first word's previus pos and end pos for last word's next pos\n",
    "                    if(i==0):\n",
    "                        prev_pos='start'\n",
    "                    else:\n",
    "                        prev_pos=p[i-1]\n",
    "                        \n",
    "                    if(i==(len(s)-1)):\n",
    "                        next_pos='end'\n",
    "                    else:\n",
    "                        next_pos=p[i+1]\n",
    "                        \n",
    "                    train.append(get_train_tuple(s[i],p[i],m[i],len(s[i]),i,prev_pos,next_pos))\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoded the maxEnt classifer using the tuples created in getTrain function\n",
    "#Use the encoded part to train the MaxentClassifier\n",
    "train = getTrain()\n",
    "def train_maxEnt():\n",
    "    global train\n",
    "    encoding = maxent.TypedMaxentFeatureEncoding.train(\n",
    "    train, count_cutoff=3, alwayson_features=True)\n",
    "    classifier = maxent.MaxentClassifier.train(\n",
    "    train, bernoulli=False, encoding=encoding, trace=0,max_iter=15)\n",
    "    return classifier\n",
    "classifier = train_maxEnt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tuples to be passed to the Viterbi Function\n",
    "def getTest(filePath):\n",
    "    test = []\n",
    "    with open(filePath) as csvfile:\n",
    "            readCSV = csv.reader(csvfile, delimiter=',')\n",
    "            next(readCSV, None)\n",
    "            for row in readCSV:\n",
    "                s=row[0].replace('[',' ').replace(']',' ').split()\n",
    "                #m=[int(i) for i in row[2].replace('[',' ').replace(',',' ').replace(']',' ').split()]\n",
    "                p = row[1].replace('[',' ').replace(',',' ').replace(']',' ').replace('\\'','').split()\n",
    "                for i in range(len(s)):\n",
    "                    \n",
    "                    if(i==0):\n",
    "                        prev_pos='start'\n",
    "                    else:\n",
    "                        prev_pos=p[i-1]\n",
    "                        \n",
    "                    if(i==(len(s)-1)):\n",
    "                        next_pos='end'\n",
    "                    else:\n",
    "                        next_pos=p[i+1]\n",
    "                        \n",
    "                    test.append(get_test_dict(s[i],p[i],len(s[i]),i,prev_pos,next_pos))\n",
    "    return test\n",
    "ksValPath = 'C:/Users/hp/Desktop/Kaveesha/Cornell/Fall 2019/5740-NLP/P2_release (3)/P2_release/data_release/test_no_label.csv'\n",
    "test = getTest(ksValPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.classify_many(test)\n",
    "disb = classifier.prob_classify_many(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Viterbi\n",
    "def viterbiFuncMEMM(sentenceWhole,lexical_prob,start_1,start_0,count,file, disb):\n",
    "    sentence=sentenceWhole.split()\n",
    "    n=len(sentence)\n",
    "    score=np.zeros((2,n))\n",
    "    bptr=np.zeros((2,n), dtype=int)\n",
    "    \n",
    "    writer=open(file,\"a+\")\n",
    "    \n",
    "    features = [{},{}] #eg -> TODO\n",
    "    len_p=len(lexical_prob)\n",
    "    \n",
    "    k = 0.000001\n",
    "    \n",
    "    #Initialization\n",
    "    for i in range(2):\n",
    "        if (sentence[0],i) not in lexical_prob:\n",
    "            lexical_prob[sentence[0],i]=0\n",
    "        score[i][0]= disb[0].prob(i) * (((lexical_prob[sentence[0],i])+k)/(count[i]+k*len_p))\n",
    "        bptr[i][0] = -1\n",
    "        \n",
    "    #Iteration\n",
    "    for word in range(1,n):\n",
    "        for label in range(2):\n",
    "            scores_for_next_label=[]\n",
    "            for j in range(2):\n",
    "                Pti=disb[word].prob(label)\n",
    "                sc=score[j][word-1]*Pti\n",
    "                scores_for_next_label.append(sc)\n",
    "            if (sentence[word],label) not in lexical_prob:\n",
    "                lexical_prob[(sentence[word],label)]=0\n",
    "            Pwi=(lexical_prob[(sentence[word],label)]+k)/(count[label]+k*len_p)\n",
    "            score[label][word]=max(scores_for_next_label)*Pwi\n",
    "            bptr[label][word]=round(scores_for_next_label.index(max(scores_for_next_label)))\n",
    "            \n",
    "    #Backtracking\n",
    "    t=[-1]*n\n",
    "      \n",
    "    t[n-1] = np.where(score == (max(score[i][n-1] for i in range(2))))[0][0]\n",
    "    for i in range(n-2,-1,-1):\n",
    "        t[i]=bptr[t[i+1]][i+1]\n",
    "    for i in t:\n",
    "        writer.write(str(i)+\"\\n\")\n",
    "    writer.close()\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the code for validation and see the accuracy and fscore and precision and recall\n",
    "ksTrainPath = 'C:/Users/hp/Desktop/Kaveesha/Cornell/Fall 2019/5740-NLP/P2_release (3)/P2_release/data_release/train.csv'\n",
    "ksValPath = 'C:/Users/hp/Desktop/Kaveesha/Cornell/Fall 2019/5740-NLP/P2_release (3)/P2_release/data_release/val.csv'\n",
    "ksValOutPath = 'C:/Users/hp/Desktop/Kaveesha/Cornell/Fall 2019/5740-NLP/P2_release (3)/P2_release/data_release/valOut.txt'\n",
    "tags = []\n",
    "m = []\n",
    "def main():\n",
    "    global disb\n",
    "    lexical_prob,trans_prob,start_1,start_0,count=trainingModel(ksTrainPath)\n",
    "    testingFile=ksValPath\n",
    "    file=ksValOutPath\n",
    "    global tags\n",
    "    global m\n",
    "    with open(testingFile) as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        next(readCSV, None)\n",
    "        for row in readCSV:\n",
    "            tags.extend(viterbiFuncMEMM(row[0],lexical_prob,start_1,start_0,count,file,disb))\n",
    "            arr = [int(i) for i in row[2].replace('[',' ').replace(',',' ').replace(']',' ').split()]\n",
    "            m.extend(arr)\n",
    "    accuracy(tags,m)\n",
    "    fscore(tags,m)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
