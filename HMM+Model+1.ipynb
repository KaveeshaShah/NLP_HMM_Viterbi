{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from string import punctuation\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training the model\n",
    "def trainingModel(trainingFile):\n",
    "    lexical_prob=dict()\n",
    "    trans_prob=dict()\n",
    "    start_1=0\n",
    "    start_0=0\n",
    "    count=[]\n",
    "\n",
    "    with open(trainingFile) as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        #Ignore the heading\n",
    "        next(readCSV, None)\n",
    "        for row in readCSV:\n",
    "            #read each sentence which is row[0]\n",
    "            s=row[0].lower().replace('[',' ').replace(']',' ').split()\n",
    "            #read the metaphor labels with respect to each word which is row[2]\n",
    "            m=[int(i) for i in row[2].replace('[',' ').replace(',',' ').replace(']',' ').split()]\n",
    "            dict_iteration=dict(zip(s, m))\n",
    "            \n",
    "            #assign counts to each (word, metaphor label) pair\n",
    "            for i in dict_iteration.items():\n",
    "                if i in lexical_prob:\n",
    "                    lexical_prob[i]+=1\n",
    "                else:\n",
    "                    lexical_prob[i]=1\n",
    "            \n",
    "            #assign count of start words that are metaphor and those that are not \n",
    "            #Used for Initiaization step in Viterbi\n",
    "            if(m[0]==0):\n",
    "                start_0+=1\n",
    "            else:\n",
    "                start_1+=1\n",
    "                \n",
    "            #Check the transition and count fr each type of transition    \n",
    "            for i in range(len(m)-1):\n",
    "\n",
    "                tup=(m[i],m[i+1])\n",
    "                if tup in trans_prob:\n",
    "                    trans_prob[tup]+=1\n",
    "                else:\n",
    "                    trans_prob[tup]=1\n",
    "        #count for transitions starting from 0 and 1 respectively            \n",
    "        count.append(trans_prob[(0,0)]+trans_prob[(0,1)])\n",
    "        count.append(trans_prob[(1,0)]+trans_prob[(1,1)])\n",
    "        return lexical_prob,trans_prob,start_1,start_0,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Viterbi\n",
    "def viterbiFunc(sentenceWhole,lexical_prob,trans_prob,start_1,start_0,count,file):\n",
    "    sentence=sentenceWhole.split()\n",
    "\n",
    "    n=len(sentence)\n",
    "    score=np.zeros((2,n))\n",
    "    bptr=np.zeros((2,n), dtype=int)\n",
    "    \n",
    "    writer=open(file,\"a+\")\n",
    "    len_p=len(lexical_prob)\n",
    "    \n",
    "    k = 0.00001\n",
    "    \n",
    "    #Initialization\n",
    "    for i in range(2):\n",
    "        #if a word, label pair not found, assign a 0 count to it. it will be handled by smoothing \n",
    "        if (sentence[0],i) not in lexical_prob:\n",
    "            lexical_prob[sentence[0],i]=0\n",
    "        score[i][0]=(start_0/(start_0+start_1)) * (((lexical_prob[sentence[0],i])+k)/(count[i]+k*len_p))\n",
    "        bptr[i][0] = -1\n",
    "        \n",
    "    #Iteration\n",
    "    for word in range(1,n):\n",
    "        for label in range(2):\n",
    "            scores_for_next_label=[]\n",
    "            for j in range(2):\n",
    "                Pti=trans_prob[(j,label)]/count[j]\n",
    "                sc=score[j][word-1]*Pti\n",
    "                scores_for_next_label.append(sc)\n",
    "            if (sentence[word],label) not in lexical_prob:\n",
    "                lexical_prob[(sentence[word],label)]=0\n",
    "            Pwi=(lexical_prob[(sentence[word],label)]+k)/(count[label]+k*len_p)\n",
    "            score[label][word]=max(scores_for_next_label)*Pwi\n",
    "            bptr[label][word]=round(scores_for_next_label.index(max(scores_for_next_label)))\n",
    "            \n",
    "    #Backtracking\n",
    "    t=[-1]*n\n",
    "      \n",
    "    t[n-1] = np.where(score == (max(score[i][n-1] for i in range(2))))[0][0]\n",
    "    for i in range(n-2,-1,-1):\n",
    "        t[i]=bptr[t[i+1]][i+1]\n",
    "    for i in t:\n",
    "        writer.write(str(i)+\"\\n\")\n",
    "    writer.close()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(output,actual):\n",
    "    print(accuracy_score(actual,output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fscore(output,actual):\n",
    "    print(precision_recall_fscore_support(actual, output,average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): 87135, (0, 1): 10169, (1, 0): 10606, (1, 1): 2389}\n",
      "0.886300093197\n",
      "0.493892601982\n",
      "(0.51133381054640892, 0.47760196122130599, 0.49389260198202345, None)\n"
     ]
    }
   ],
   "source": [
    "ksTrainPath = 'C:/Users/hp/Desktop/Kaveesha/Cornell/Fall 2019/5740-NLP/P2_release (3)/P2_release/data_release/train.csv'\n",
    "ksValPath = 'C:/Users/hp/Desktop/Kaveesha/Cornell/Fall 2019/5740-NLP/P2_release (3)/P2_release/data_release/val.csv'\n",
    "ksValOutPath = 'C:/Users/hp/Desktop/Kaveesha/Cornell/Fall 2019/5740-NLP/P2_release (3)/P2_release/data_release/valOut.txt'\n",
    "def main():\n",
    "    lexical_prob,trans_prob,start_1,start_0,count=trainingModel(ksTrainPath)\n",
    "    testingFile=ksValPath\n",
    "    file=ksValOutPath\n",
    "    tags = []\n",
    "    m =[]\n",
    "    sum12=0\n",
    "    with open(testingFile) as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        next(readCSV, None)\n",
    "        for row in readCSV:\n",
    "            #tags=Predicted values by Viterbi \n",
    "            tags.extend(viterbiFunc(row[0].lower(),lexical_prob,trans_prob,start_1,start_0,count,file))\n",
    "            arr = [int(i) for i in row[2].replace('[',' ').replace(',',' ').replace(']',' ').split()]\n",
    "            #m= Actual values in the train data\n",
    "            m.extend(arr)\n",
    "    accuracy(tags,m)   \n",
    "    fscore(tags,m)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
